{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import yaml\n",
    "from ludwig import LudwigModel\n",
    "import copy\n",
    "import ray\n",
    "from ludwig.utils.misc import merge_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benmackenzie/projects/Teradata/ludwig/venv/lib/python3.6/site-packages/ipykernel_launcher.py:2: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "with open(\"titanic_full.yaml\", 'r') as stream:\n",
    "    base_model = yaml.load(stream)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base should contain special markup so we know what parameters need to be updated.\n",
    "\n",
    "def build_trial(base, config):\n",
    "    combiner = base['combiner']\n",
    "    training = base['training']\n",
    "    \n",
    "    \n",
    "    combiner = merge_dict(combiner, {'num_fc_layers': config['num_fc_layers']})\n",
    "    training = merge_dict(training, {'batch_size': config['batch_size']})\n",
    "    \n",
    "    new_model_def = {'input_features': base['input_features'], \n",
    "                 'output_features': base['output_features'], \n",
    "                 'combiner': combiner, \n",
    "                 'training': training}\n",
    "    \n",
    "    return new_model_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '/Users/benmackenzie/projects/Teradata/ludwig/examples/hyperparameters/titanic.hdf5'\n",
    "metadata = '/Users/benmackenzie/projects/Teradata/ludwig/examples/hyperparameters/titanic.json'\n",
    "\n",
    "def train(base, config, reporter):\n",
    "    \n",
    "    new_model_def = build_trial(base, config)\n",
    "    model = LudwigModel(new_model_def)\n",
    "    train_stats = model.train(data_hdf5=data, train_set_metadata_json=metadata)\n",
    "    return reporter(mean_accuracy=train_stats['validation']['Survived']['accuracy'][-1], done=True)\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-14 09:13:03,744\tWARNING worker.py:1381 -- WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.\n",
      "2019-03-14 09:13:03,747\tINFO node.py:439 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-03-14_09-13-03_20306/logs.\n",
      "2019-03-14 09:13:03,869\tINFO services.py:364 -- Waiting for redis server at 127.0.0.1:13451 to respond...\n",
      "2019-03-14 09:13:04,027\tINFO services.py:364 -- Waiting for redis server at 127.0.0.1:60162 to respond...\n",
      "2019-03-14 09:13:04,032\tINFO services.py:761 -- Starting Redis shard with 1.72 GB max memory.\n",
      "2019-03-14 09:13:04,088\tINFO services.py:1449 -- Starting the Plasma object store with 2.58 GB memory using /tmp.\n",
      "2019-03-14 09:13:04,692\tINFO tune.py:135 -- Tip: to resume incomplete experiments, pass resume='prompt' or resume=True to run_experiments()\n",
      "2019-03-14 09:13:04,694\tINFO tune.py:145 -- Starting a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/4 CPUs, 0/0 GPUs\n",
      "Unknown memory usage. Please run `pip install psutil` (or ray[debug]) to resolve)\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/4 CPUs, 0/0 GPUs\n",
      "Unknown memory usage. Please run `pip install psutil` (or ray[debug]) to resolve)\n",
      "Result logdir: /Users/benmackenzie/ray_results/my_experiment\n",
      "Number of trials: 20 ({'RUNNING': 1, 'PENDING': 19})\n",
      "PENDING trials:\n",
      " - train_1_batch_size=16,num_fc_layers=1:\tPENDING\n",
      " - train_2_batch_size=32,num_fc_layers=1:\tPENDING\n",
      " - train_3_batch_size=64,num_fc_layers=1:\tPENDING\n",
      " - train_4_batch_size=128,num_fc_layers=1:\tPENDING\n",
      " - train_5_batch_size=4,num_fc_layers=2:\tPENDING\n",
      " - train_6_batch_size=16,num_fc_layers=2:\tPENDING\n",
      " - train_7_batch_size=32,num_fc_layers=2:\tPENDING\n",
      " - train_8_batch_size=64,num_fc_layers=2:\tPENDING\n",
      " - train_9_batch_size=128,num_fc_layers=2:\tPENDING\n",
      " - train_10_batch_size=4,num_fc_layers=3:\tPENDING\n",
      " - train_11_batch_size=16,num_fc_layers=3:\tPENDING\n",
      " - train_12_batch_size=32,num_fc_layers=3:\tPENDING\n",
      " - train_13_batch_size=64,num_fc_layers=3:\tPENDING\n",
      " - train_14_batch_size=128,num_fc_layers=3:\tPENDING\n",
      " - train_15_batch_size=4,num_fc_layers=4:\tPENDING\n",
      " - train_16_batch_size=16,num_fc_layers=4:\tPENDING\n",
      " - train_17_batch_size=32,num_fc_layers=4:\tPENDING\n",
      " - train_18_batch_size=64,num_fc_layers=4:\tPENDING\n",
      " - train_19_batch_size=128,num_fc_layers=4:\tPENDING\n",
      "RUNNING trials:\n",
      " - train_0_batch_size=4,num_fc_layers=1:\tRUNNING\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=20716)\u001b[0m /Users/benmackenzie/projects/Teradata/ludwig/venv/lib/python3.6/site-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "\u001b[2m\u001b[36m(pid=20716)\u001b[0m   \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
      "\u001b[2m\u001b[36m(pid=20715)\u001b[0m /Users/benmackenzie/projects/Teradata/ludwig/venv/lib/python3.6/site-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "\u001b[2m\u001b[36m(pid=20715)\u001b[0m   \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
      "\u001b[2m\u001b[36m(pid=20714)\u001b[0m /Users/benmackenzie/projects/Teradata/ludwig/venv/lib/python3.6/site-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "\u001b[2m\u001b[36m(pid=20714)\u001b[0m   \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
      "\u001b[2m\u001b[36m(pid=20717)\u001b[0m /Users/benmackenzie/projects/Teradata/ludwig/venv/lib/python3.6/site-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "\u001b[2m\u001b[36m(pid=20717)\u001b[0m   \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
      "Result for train_1_batch_size=16,num_fc_layers=1:\n",
      "  date: 2019-03-14_09-13-31\n",
      "  done: true\n",
      "  experiment_id: 9a22aceda1134c97aaa0979d090b99ad\n",
      "  hostname: 192.168.219.94\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.8024691358024691\n",
      "  node_ip: 192.168.219.94\n",
      "  pid: 20714\n",
      "  time_since_restore: 8.037699937820435\n",
      "  time_this_iter_s: 8.037699937820435\n",
      "  time_total_s: 8.037699937820435\n",
      "  timestamp: 1552569211\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/4 CPUs, 0/0 GPUs\n",
      "Unknown memory usage. Please run `pip install psutil` (or ray[debug]) to resolve)\n",
      "Result logdir: /Users/benmackenzie/ray_results/my_experiment\n",
      "Number of trials: 20 ({'RUNNING': 3, 'TERMINATED': 1, 'PENDING': 16})\n",
      "PENDING trials:\n",
      " - train_4_batch_size=128,num_fc_layers=1:\tPENDING\n",
      " - train_5_batch_size=4,num_fc_layers=2:\tPENDING\n",
      " - train_6_batch_size=16,num_fc_layers=2:\tPENDING\n",
      " - train_7_batch_size=32,num_fc_layers=2:\tPENDING\n",
      " - train_8_batch_size=64,num_fc_layers=2:\tPENDING\n",
      " - train_9_batch_size=128,num_fc_layers=2:\tPENDING\n",
      " - train_10_batch_size=4,num_fc_layers=3:\tPENDING\n",
      " - train_11_batch_size=16,num_fc_layers=3:\tPENDING\n",
      " - train_12_batch_size=32,num_fc_layers=3:\tPENDING\n",
      " - train_13_batch_size=64,num_fc_layers=3:\tPENDING\n",
      " - train_14_batch_size=128,num_fc_layers=3:\tPENDING\n",
      " - train_15_batch_size=4,num_fc_layers=4:\tPENDING\n",
      " - train_16_batch_size=16,num_fc_layers=4:\tPENDING\n",
      " - train_17_batch_size=32,num_fc_layers=4:\tPENDING\n",
      " - train_18_batch_size=64,num_fc_layers=4:\tPENDING\n",
      " - train_19_batch_size=128,num_fc_layers=4:\tPENDING\n",
      "RUNNING trials:\n",
      " - train_0_batch_size=4,num_fc_layers=1:\tRUNNING\n",
      " - train_2_batch_size=32,num_fc_layers=1:\tRUNNING\n",
      " - train_3_batch_size=64,num_fc_layers=1:\tRUNNING\n",
      "TERMINATED trials:\n",
      " - train_1_batch_size=16,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20714], 8 s, 1 iter, 0.802 acc\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=20714)\u001b[0m WARNING: Not monitoring node memory since `psutil` is not installed. Install this with `pip install psutil` (or ray[debug]) to enable debugging of memory-related crashes.\n",
      "Result for train_2_batch_size=32,num_fc_layers=1:\n",
      "  date: 2019-03-14_09-13-32\n",
      "  done: true\n",
      "  experiment_id: c35161f68bc74e27b9cec8366c82d221\n",
      "  hostname: 192.168.219.94\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.8024691358024691\n",
      "  node_ip: 192.168.219.94\n",
      "  pid: 20715\n",
      "  time_since_restore: 9.021356105804443\n",
      "  time_this_iter_s: 9.021356105804443\n",
      "  time_total_s: 9.021356105804443\n",
      "  timestamp: 1552569212\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=20715)\u001b[0m WARNING: Not monitoring node memory since `psutil` is not installed. Install this with `pip install psutil` (or ray[debug]) to enable debugging of memory-related crashes.\n",
      "Result for train_0_batch_size=4,num_fc_layers=1:\n",
      "  date: 2019-03-14_09-13-34\n",
      "  done: true\n",
      "  experiment_id: 5c19a3185a7a4edb839da9ebcc60dcd9\n",
      "  hostname: 192.168.219.94\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.7901234567901234\n",
      "  node_ip: 192.168.219.94\n",
      "  pid: 20716\n",
      "  time_since_restore: 11.020610809326172\n",
      "  time_this_iter_s: 11.020610809326172\n",
      "  time_total_s: 11.020610809326172\n",
      "  timestamp: 1552569214\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=20716)\u001b[0m WARNING: Not monitoring node memory since `psutil` is not installed. Install this with `pip install psutil` (or ray[debug]) to enable debugging of memory-related crashes.\n",
      "Result for train_3_batch_size=64,num_fc_layers=1:\n",
      "  date: 2019-03-14_09-13-35\n",
      "  done: true\n",
      "  experiment_id: 3bfaad1da0154a17a52e975249d7cb7a\n",
      "  hostname: 192.168.219.94\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.8148148148148148\n",
      "  node_ip: 192.168.219.94\n",
      "  pid: 20717\n",
      "  time_since_restore: 12.051559925079346\n",
      "  time_this_iter_s: 12.051559925079346\n",
      "  time_total_s: 12.051559925079346\n",
      "  timestamp: 1552569215\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=20717)\u001b[0m WARNING: Not monitoring node memory since `psutil` is not installed. Install this with `pip install psutil` (or ray[debug]) to enable debugging of memory-related crashes.\n",
      "\u001b[2m\u001b[36m(pid=20718)\u001b[0m /Users/benmackenzie/projects/Teradata/ludwig/venv/lib/python3.6/site-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "\u001b[2m\u001b[36m(pid=20718)\u001b[0m   \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
      "\u001b[2m\u001b[36m(pid=20720)\u001b[0m /Users/benmackenzie/projects/Teradata/ludwig/venv/lib/python3.6/site-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "\u001b[2m\u001b[36m(pid=20720)\u001b[0m   \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
      "\u001b[2m\u001b[36m(pid=20719)\u001b[0m /Users/benmackenzie/projects/Teradata/ludwig/venv/lib/python3.6/site-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "\u001b[2m\u001b[36m(pid=20719)\u001b[0m   \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
      "\u001b[2m\u001b[36m(pid=20721)\u001b[0m /Users/benmackenzie/projects/Teradata/ludwig/venv/lib/python3.6/site-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "\u001b[2m\u001b[36m(pid=20721)\u001b[0m   \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
      "Result for train_6_batch_size=16,num_fc_layers=2:\n",
      "  date: 2019-03-14_09-13-58\n",
      "  done: true\n",
      "  experiment_id: 254ac909f7244f62a4db331520b4fc6e\n",
      "  hostname: 192.168.219.94\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.7901234567901234\n",
      "  node_ip: 192.168.219.94\n",
      "  pid: 20720\n",
      "  time_since_restore: 8.038987874984741\n",
      "  time_this_iter_s: 8.038987874984741\n",
      "  time_total_s: 8.038987874984741\n",
      "  timestamp: 1552569238\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/4 CPUs, 0/0 GPUs\n",
      "Unknown memory usage. Please run `pip install psutil` (or ray[debug]) to resolve)\n",
      "Result logdir: /Users/benmackenzie/ray_results/my_experiment\n",
      "Number of trials: 20 ({'TERMINATED': 5, 'RUNNING': 3, 'PENDING': 12})\n",
      "PENDING trials:\n",
      " - train_8_batch_size=64,num_fc_layers=2:\tPENDING\n",
      " - train_9_batch_size=128,num_fc_layers=2:\tPENDING\n",
      " - train_10_batch_size=4,num_fc_layers=3:\tPENDING\n",
      " - train_11_batch_size=16,num_fc_layers=3:\tPENDING\n",
      " - train_12_batch_size=32,num_fc_layers=3:\tPENDING\n",
      " - train_13_batch_size=64,num_fc_layers=3:\tPENDING\n",
      " - train_14_batch_size=128,num_fc_layers=3:\tPENDING\n",
      " - train_15_batch_size=4,num_fc_layers=4:\tPENDING\n",
      " - train_16_batch_size=16,num_fc_layers=4:\tPENDING\n",
      " - train_17_batch_size=32,num_fc_layers=4:\tPENDING\n",
      " - train_18_batch_size=64,num_fc_layers=4:\tPENDING\n",
      " - train_19_batch_size=128,num_fc_layers=4:\tPENDING\n",
      "RUNNING trials:\n",
      " - train_4_batch_size=128,num_fc_layers=1:\tRUNNING\n",
      " - train_5_batch_size=4,num_fc_layers=2:\tRUNNING\n",
      " - train_7_batch_size=32,num_fc_layers=2:\tRUNNING\n",
      "TERMINATED trials:\n",
      " - train_0_batch_size=4,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20716], 11 s, 1 iter, 0.79 acc\n",
      " - train_1_batch_size=16,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20714], 8 s, 1 iter, 0.802 acc\n",
      " - train_2_batch_size=32,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20715], 9 s, 1 iter, 0.802 acc\n",
      " - train_3_batch_size=64,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20717], 12 s, 1 iter, 0.815 acc\n",
      " - train_6_batch_size=16,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20720], 8 s, 1 iter, 0.79 acc\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=20720)\u001b[0m WARNING: Not monitoring node memory since `psutil` is not installed. Install this with `pip install psutil` (or ray[debug]) to enable debugging of memory-related crashes.\n",
      "Result for train_7_batch_size=32,num_fc_layers=2:\n",
      "  date: 2019-03-14_09-14-03\n",
      "  done: true\n",
      "  experiment_id: d33d20245cfa4307aca43f4d4cf3f6f9\n",
      "  hostname: 192.168.219.94\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.7901234567901234\n",
      "  node_ip: 192.168.219.94\n",
      "  pid: 20718\n",
      "  time_since_restore: 13.152106046676636\n",
      "  time_this_iter_s: 13.152106046676636\n",
      "  time_total_s: 13.152106046676636\n",
      "  timestamp: 1552569243\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/4 CPUs, 0/0 GPUs\n",
      "Unknown memory usage. Please run `pip install psutil` (or ray[debug]) to resolve)\n",
      "Result logdir: /Users/benmackenzie/ray_results/my_experiment\n",
      "Number of trials: 20 ({'TERMINATED': 6, 'RUNNING': 3, 'PENDING': 11})\n",
      "PENDING trials:\n",
      " - train_9_batch_size=128,num_fc_layers=2:\tPENDING\n",
      " - train_10_batch_size=4,num_fc_layers=3:\tPENDING\n",
      " - train_11_batch_size=16,num_fc_layers=3:\tPENDING\n",
      " - train_12_batch_size=32,num_fc_layers=3:\tPENDING\n",
      " - train_13_batch_size=64,num_fc_layers=3:\tPENDING\n",
      " - train_14_batch_size=128,num_fc_layers=3:\tPENDING\n",
      " - train_15_batch_size=4,num_fc_layers=4:\tPENDING\n",
      " - train_16_batch_size=16,num_fc_layers=4:\tPENDING\n",
      " - train_17_batch_size=32,num_fc_layers=4:\tPENDING\n",
      " - train_18_batch_size=64,num_fc_layers=4:\tPENDING\n",
      " - train_19_batch_size=128,num_fc_layers=4:\tPENDING\n",
      "RUNNING trials:\n",
      " - train_4_batch_size=128,num_fc_layers=1:\tRUNNING\n",
      " - train_5_batch_size=4,num_fc_layers=2:\tRUNNING\n",
      " - train_8_batch_size=64,num_fc_layers=2:\tRUNNING\n",
      "TERMINATED trials:\n",
      " - train_0_batch_size=4,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20716], 11 s, 1 iter, 0.79 acc\n",
      " - train_1_batch_size=16,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20714], 8 s, 1 iter, 0.802 acc\n",
      " - train_2_batch_size=32,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20715], 9 s, 1 iter, 0.802 acc\n",
      " - train_3_batch_size=64,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20717], 12 s, 1 iter, 0.815 acc\n",
      " - train_6_batch_size=16,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20720], 8 s, 1 iter, 0.79 acc\n",
      " - train_7_batch_size=32,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20718], 13 s, 1 iter, 0.79 acc\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=20718)\u001b[0m WARNING: Not monitoring node memory since `psutil` is not installed. Install this with `pip install psutil` (or ray[debug]) to enable debugging of memory-related crashes.\n",
      "Result for train_5_batch_size=4,num_fc_layers=2:\n",
      "  date: 2019-03-14_09-14-04\n",
      "  done: true\n",
      "  experiment_id: e30d79cd75e44151bd28bd5c021994f6\n",
      "  hostname: 192.168.219.94\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.8024691358024691\n",
      "  node_ip: 192.168.219.94\n",
      "  pid: 20719\n",
      "  time_since_restore: 14.160902976989746\n",
      "  time_this_iter_s: 14.160902976989746\n",
      "  time_total_s: 14.160902976989746\n",
      "  timestamp: 1552569244\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=20719)\u001b[0m WARNING: Not monitoring node memory since `psutil` is not installed. Install this with `pip install psutil` (or ray[debug]) to enable debugging of memory-related crashes.\n",
      "Result for train_4_batch_size=128,num_fc_layers=1:\n",
      "  date: 2019-03-14_09-14-07\n",
      "  done: true\n",
      "  experiment_id: b49460c5e7094d83b628cbf516d82c9c\n",
      "  hostname: 192.168.219.94\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.8024691358024691\n",
      "  node_ip: 192.168.219.94\n",
      "  pid: 20721\n",
      "  time_since_restore: 17.10904288291931\n",
      "  time_this_iter_s: 17.10904288291931\n",
      "  time_total_s: 17.10904288291931\n",
      "  timestamp: 1552569247\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=20721)\u001b[0m WARNING: Not monitoring node memory since `psutil` is not installed. Install this with `pip install psutil` (or ray[debug]) to enable debugging of memory-related crashes.\n",
      "\u001b[2m\u001b[36m(pid=20755)\u001b[0m /Users/benmackenzie/projects/Teradata/ludwig/venv/lib/python3.6/site-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "\u001b[2m\u001b[36m(pid=20755)\u001b[0m   \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
      "\u001b[2m\u001b[36m(pid=20756)\u001b[0m /Users/benmackenzie/projects/Teradata/ludwig/venv/lib/python3.6/site-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "\u001b[2m\u001b[36m(pid=20756)\u001b[0m   \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
      "\u001b[2m\u001b[36m(pid=20760)\u001b[0m /Users/benmackenzie/projects/Teradata/ludwig/venv/lib/python3.6/site-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "\u001b[2m\u001b[36m(pid=20760)\u001b[0m   \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
      "\u001b[2m\u001b[36m(pid=20763)\u001b[0m /Users/benmackenzie/projects/Teradata/ludwig/venv/lib/python3.6/site-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "\u001b[2m\u001b[36m(pid=20763)\u001b[0m   \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
      "Result for train_8_batch_size=64,num_fc_layers=2:\n",
      "  date: 2019-03-14_09-14-23\n",
      "  done: true\n",
      "  experiment_id: f8ca001a90f94e0fb4b1482f7babd13d\n",
      "  hostname: 192.168.219.94\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.7654320987654321\n",
      "  node_ip: 192.168.219.94\n",
      "  pid: 20755\n",
      "  time_since_restore: 9.058722019195557\n",
      "  time_this_iter_s: 9.058722019195557\n",
      "  time_total_s: 9.058722019195557\n",
      "  timestamp: 1552569263\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/4 CPUs, 0/0 GPUs\n",
      "Unknown memory usage. Please run `pip install psutil` (or ray[debug]) to resolve)\n",
      "Result logdir: /Users/benmackenzie/ray_results/my_experiment\n",
      "Number of trials: 20 ({'TERMINATED': 9, 'RUNNING': 3, 'PENDING': 8})\n",
      "PENDING trials:\n",
      " - train_12_batch_size=32,num_fc_layers=3:\tPENDING\n",
      " - train_13_batch_size=64,num_fc_layers=3:\tPENDING\n",
      " - train_14_batch_size=128,num_fc_layers=3:\tPENDING\n",
      " - train_15_batch_size=4,num_fc_layers=4:\tPENDING\n",
      " - train_16_batch_size=16,num_fc_layers=4:\tPENDING\n",
      " - train_17_batch_size=32,num_fc_layers=4:\tPENDING\n",
      " - train_18_batch_size=64,num_fc_layers=4:\tPENDING\n",
      " - train_19_batch_size=128,num_fc_layers=4:\tPENDING\n",
      "RUNNING trials:\n",
      " - train_9_batch_size=128,num_fc_layers=2:\tRUNNING\n",
      " - train_10_batch_size=4,num_fc_layers=3:\tRUNNING\n",
      " - train_11_batch_size=16,num_fc_layers=3:\tRUNNING\n",
      "TERMINATED trials:\n",
      " - train_0_batch_size=4,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20716], 11 s, 1 iter, 0.79 acc\n",
      " - train_1_batch_size=16,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20714], 8 s, 1 iter, 0.802 acc\n",
      " - train_2_batch_size=32,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20715], 9 s, 1 iter, 0.802 acc\n",
      " - train_3_batch_size=64,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20717], 12 s, 1 iter, 0.815 acc\n",
      " - train_4_batch_size=128,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20721], 17 s, 1 iter, 0.802 acc\n",
      " - train_5_batch_size=4,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20719], 14 s, 1 iter, 0.802 acc\n",
      " - train_6_batch_size=16,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20720], 8 s, 1 iter, 0.79 acc\n",
      " - train_7_batch_size=32,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20718], 13 s, 1 iter, 0.79 acc\n",
      " - train_8_batch_size=64,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20755], 9 s, 1 iter, 0.765 acc\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=20755)\u001b[0m WARNING: Not monitoring node memory since `psutil` is not installed. Install this with `pip install psutil` (or ray[debug]) to enable debugging of memory-related crashes.\n",
      "Result for train_9_batch_size=128,num_fc_layers=2:\n",
      "  date: 2019-03-14_09-14-27\n",
      "  done: true\n",
      "  experiment_id: 343a7c02d9e549d78b408758b121cd49\n",
      "  hostname: 192.168.219.94\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.7654320987654321\n",
      "  node_ip: 192.168.219.94\n",
      "  pid: 20756\n",
      "  time_since_restore: 12.131294012069702\n",
      "  time_this_iter_s: 12.131294012069702\n",
      "  time_total_s: 12.131294012069702\n",
      "  timestamp: 1552569267\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=20756)\u001b[0m WARNING: Not monitoring node memory since `psutil` is not installed. Install this with `pip install psutil` (or ray[debug]) to enable debugging of memory-related crashes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_11_batch_size=16,num_fc_layers=3:\n",
      "  date: 2019-03-14_09-14-29\n",
      "  done: true\n",
      "  experiment_id: 99271892d709460ca61b5991a6d98b7f\n",
      "  hostname: 192.168.219.94\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.7777777777777778\n",
      "  node_ip: 192.168.219.94\n",
      "  pid: 20763\n",
      "  time_since_restore: 10.053380250930786\n",
      "  time_this_iter_s: 10.053380250930786\n",
      "  time_total_s: 10.053380250930786\n",
      "  timestamp: 1552569269\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/4 CPUs, 0/0 GPUs\n",
      "Unknown memory usage. Please run `pip install psutil` (or ray[debug]) to resolve)\n",
      "Result logdir: /Users/benmackenzie/ray_results/my_experiment\n",
      "Number of trials: 20 ({'TERMINATED': 11, 'RUNNING': 3, 'PENDING': 6})\n",
      "PENDING trials:\n",
      " - train_14_batch_size=128,num_fc_layers=3:\tPENDING\n",
      " - train_15_batch_size=4,num_fc_layers=4:\tPENDING\n",
      " - train_16_batch_size=16,num_fc_layers=4:\tPENDING\n",
      " - train_17_batch_size=32,num_fc_layers=4:\tPENDING\n",
      " - train_18_batch_size=64,num_fc_layers=4:\tPENDING\n",
      " - train_19_batch_size=128,num_fc_layers=4:\tPENDING\n",
      "RUNNING trials:\n",
      " - train_10_batch_size=4,num_fc_layers=3:\tRUNNING\n",
      " - train_12_batch_size=32,num_fc_layers=3:\tRUNNING\n",
      " - train_13_batch_size=64,num_fc_layers=3:\tRUNNING\n",
      "TERMINATED trials:\n",
      " - train_0_batch_size=4,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20716], 11 s, 1 iter, 0.79 acc\n",
      " - train_1_batch_size=16,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20714], 8 s, 1 iter, 0.802 acc\n",
      " - train_2_batch_size=32,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20715], 9 s, 1 iter, 0.802 acc\n",
      " - train_3_batch_size=64,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20717], 12 s, 1 iter, 0.815 acc\n",
      " - train_4_batch_size=128,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20721], 17 s, 1 iter, 0.802 acc\n",
      " - train_5_batch_size=4,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20719], 14 s, 1 iter, 0.802 acc\n",
      " - train_6_batch_size=16,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20720], 8 s, 1 iter, 0.79 acc\n",
      " - train_7_batch_size=32,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20718], 13 s, 1 iter, 0.79 acc\n",
      " - train_8_batch_size=64,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20755], 9 s, 1 iter, 0.765 acc\n",
      " - train_9_batch_size=128,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20756], 12 s, 1 iter, 0.765 acc\n",
      " - train_11_batch_size=16,num_fc_layers=3:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20763], 10 s, 1 iter, 0.778 acc\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=20763)\u001b[0m WARNING: Not monitoring node memory since `psutil` is not installed. Install this with `pip install psutil` (or ray[debug]) to enable debugging of memory-related crashes.\n",
      "Result for train_10_batch_size=4,num_fc_layers=3:\n",
      "  date: 2019-03-14_09-14-36\n",
      "  done: true\n",
      "  experiment_id: 11cf6b23b5c64a5896215a1b0ecc869f\n",
      "  hostname: 192.168.219.94\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.7901234567901234\n",
      "  node_ip: 192.168.219.94\n",
      "  pid: 20760\n",
      "  time_since_restore: 19.07293391227722\n",
      "  time_this_iter_s: 19.07293391227722\n",
      "  time_total_s: 19.07293391227722\n",
      "  timestamp: 1552569276\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/4 CPUs, 0/0 GPUs\n",
      "Unknown memory usage. Please run `pip install psutil` (or ray[debug]) to resolve)\n",
      "Result logdir: /Users/benmackenzie/ray_results/my_experiment\n",
      "Number of trials: 20 ({'TERMINATED': 12, 'RUNNING': 3, 'PENDING': 5})\n",
      "PENDING trials:\n",
      " - train_15_batch_size=4,num_fc_layers=4:\tPENDING\n",
      " - train_16_batch_size=16,num_fc_layers=4:\tPENDING\n",
      " - train_17_batch_size=32,num_fc_layers=4:\tPENDING\n",
      " - train_18_batch_size=64,num_fc_layers=4:\tPENDING\n",
      " - train_19_batch_size=128,num_fc_layers=4:\tPENDING\n",
      "RUNNING trials:\n",
      " - train_12_batch_size=32,num_fc_layers=3:\tRUNNING\n",
      " - train_13_batch_size=64,num_fc_layers=3:\tRUNNING\n",
      " - train_14_batch_size=128,num_fc_layers=3:\tRUNNING\n",
      "TERMINATED trials:\n",
      " - train_0_batch_size=4,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20716], 11 s, 1 iter, 0.79 acc\n",
      " - train_1_batch_size=16,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20714], 8 s, 1 iter, 0.802 acc\n",
      " - train_2_batch_size=32,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20715], 9 s, 1 iter, 0.802 acc\n",
      " - train_3_batch_size=64,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20717], 12 s, 1 iter, 0.815 acc\n",
      " - train_4_batch_size=128,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20721], 17 s, 1 iter, 0.802 acc\n",
      " - train_5_batch_size=4,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20719], 14 s, 1 iter, 0.802 acc\n",
      " - train_6_batch_size=16,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20720], 8 s, 1 iter, 0.79 acc\n",
      " - train_7_batch_size=32,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20718], 13 s, 1 iter, 0.79 acc\n",
      " - train_8_batch_size=64,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20755], 9 s, 1 iter, 0.765 acc\n",
      " - train_9_batch_size=128,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20756], 12 s, 1 iter, 0.765 acc\n",
      " - train_10_batch_size=4,num_fc_layers=3:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20760], 19 s, 1 iter, 0.79 acc\n",
      " - train_11_batch_size=16,num_fc_layers=3:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20763], 10 s, 1 iter, 0.778 acc\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=20760)\u001b[0m WARNING: Not monitoring node memory since `psutil` is not installed. Install this with `pip install psutil` (or ray[debug]) to enable debugging of memory-related crashes.\n",
      "\u001b[2m\u001b[36m(pid=20776)\u001b[0m /Users/benmackenzie/projects/Teradata/ludwig/venv/lib/python3.6/site-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "\u001b[2m\u001b[36m(pid=20776)\u001b[0m   \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
      "\u001b[2m\u001b[36m(pid=20777)\u001b[0m /Users/benmackenzie/projects/Teradata/ludwig/venv/lib/python3.6/site-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "\u001b[2m\u001b[36m(pid=20777)\u001b[0m   \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
      "\u001b[2m\u001b[36m(pid=20783)\u001b[0m /Users/benmackenzie/projects/Teradata/ludwig/venv/lib/python3.6/site-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "\u001b[2m\u001b[36m(pid=20783)\u001b[0m   \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
      "Result for train_12_batch_size=32,num_fc_layers=3:\n",
      "  date: 2019-03-14_09-14-48\n",
      "  done: true\n",
      "  experiment_id: b2d53839e2c7443f96cb97174a3eb3bf\n",
      "  hostname: 192.168.219.94\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.7901234567901234\n",
      "  node_ip: 192.168.219.94\n",
      "  pid: 20776\n",
      "  time_since_restore: 9.038381814956665\n",
      "  time_this_iter_s: 9.038381814956665\n",
      "  time_total_s: 9.038381814956665\n",
      "  timestamp: 1552569288\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/4 CPUs, 0/0 GPUs\n",
      "Unknown memory usage. Please run `pip install psutil` (or ray[debug]) to resolve)\n",
      "Result logdir: /Users/benmackenzie/ray_results/my_experiment\n",
      "Number of trials: 20 ({'TERMINATED': 13, 'RUNNING': 3, 'PENDING': 4})\n",
      "PENDING trials:\n",
      " - train_16_batch_size=16,num_fc_layers=4:\tPENDING\n",
      " - train_17_batch_size=32,num_fc_layers=4:\tPENDING\n",
      " - train_18_batch_size=64,num_fc_layers=4:\tPENDING\n",
      " - train_19_batch_size=128,num_fc_layers=4:\tPENDING\n",
      "RUNNING trials:\n",
      " - train_13_batch_size=64,num_fc_layers=3:\tRUNNING\n",
      " - train_14_batch_size=128,num_fc_layers=3:\tRUNNING\n",
      " - train_15_batch_size=4,num_fc_layers=4:\tRUNNING\n",
      "TERMINATED trials:\n",
      " - train_0_batch_size=4,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20716], 11 s, 1 iter, 0.79 acc\n",
      " - train_1_batch_size=16,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20714], 8 s, 1 iter, 0.802 acc\n",
      " - train_2_batch_size=32,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20715], 9 s, 1 iter, 0.802 acc\n",
      " - train_3_batch_size=64,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20717], 12 s, 1 iter, 0.815 acc\n",
      " - train_4_batch_size=128,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20721], 17 s, 1 iter, 0.802 acc\n",
      " - train_5_batch_size=4,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20719], 14 s, 1 iter, 0.802 acc\n",
      " - train_6_batch_size=16,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20720], 8 s, 1 iter, 0.79 acc\n",
      " - train_7_batch_size=32,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20718], 13 s, 1 iter, 0.79 acc\n",
      " - train_8_batch_size=64,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20755], 9 s, 1 iter, 0.765 acc\n",
      " - train_9_batch_size=128,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20756], 12 s, 1 iter, 0.765 acc\n",
      " - train_10_batch_size=4,num_fc_layers=3:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20760], 19 s, 1 iter, 0.79 acc\n",
      " - train_11_batch_size=16,num_fc_layers=3:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20763], 10 s, 1 iter, 0.778 acc\n",
      " - train_12_batch_size=32,num_fc_layers=3:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20776], 9 s, 1 iter, 0.79 acc\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=20776)\u001b[0m WARNING: Not monitoring node memory since `psutil` is not installed. Install this with `pip install psutil` (or ray[debug]) to enable debugging of memory-related crashes.\n",
      "Result for train_13_batch_size=64,num_fc_layers=3:\n",
      "  date: 2019-03-14_09-14-50\n",
      "  done: true\n",
      "  experiment_id: 68f5797e46444e41975db1264f45e5cc\n",
      "  hostname: 192.168.219.94\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.7901234567901234\n",
      "  node_ip: 192.168.219.94\n",
      "  pid: 20777\n",
      "  time_since_restore: 9.044251203536987\n",
      "  time_this_iter_s: 9.044251203536987\n",
      "  time_total_s: 9.044251203536987\n",
      "  timestamp: 1552569290\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=20785)\u001b[0m /Users/benmackenzie/projects/Teradata/ludwig/venv/lib/python3.6/site-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "\u001b[2m\u001b[36m(pid=20785)\u001b[0m   \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
      "\u001b[2m\u001b[36m(pid=20777)\u001b[0m WARNING: Not monitoring node memory since `psutil` is not installed. Install this with `pip install psutil` (or ray[debug]) to enable debugging of memory-related crashes.\n",
      "Result for train_14_batch_size=128,num_fc_layers=3:\n",
      "  date: 2019-03-14_09-14-53\n",
      "  done: true\n",
      "  experiment_id: 050b10e685d04f86852bddbe44551344\n",
      "  hostname: 192.168.219.94\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.7901234567901234\n",
      "  node_ip: 192.168.219.94\n",
      "  pid: 20783\n",
      "  time_since_restore: 10.055480003356934\n",
      "  time_this_iter_s: 10.055480003356934\n",
      "  time_total_s: 10.055480003356934\n",
      "  timestamp: 1552569293\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/4 CPUs, 0/0 GPUs\n",
      "Unknown memory usage. Please run `pip install psutil` (or ray[debug]) to resolve)\n",
      "Result logdir: /Users/benmackenzie/ray_results/my_experiment\n",
      "Number of trials: 20 ({'TERMINATED': 15, 'RUNNING': 3, 'PENDING': 2})\n",
      "PENDING trials:\n",
      " - train_18_batch_size=64,num_fc_layers=4:\tPENDING\n",
      " - train_19_batch_size=128,num_fc_layers=4:\tPENDING\n",
      "RUNNING trials:\n",
      " - train_15_batch_size=4,num_fc_layers=4:\tRUNNING\n",
      " - train_16_batch_size=16,num_fc_layers=4:\tRUNNING\n",
      " - train_17_batch_size=32,num_fc_layers=4:\tRUNNING\n",
      "TERMINATED trials:\n",
      " - train_0_batch_size=4,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20716], 11 s, 1 iter, 0.79 acc\n",
      " - train_1_batch_size=16,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20714], 8 s, 1 iter, 0.802 acc\n",
      " - train_2_batch_size=32,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20715], 9 s, 1 iter, 0.802 acc\n",
      " - train_3_batch_size=64,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20717], 12 s, 1 iter, 0.815 acc\n",
      " - train_4_batch_size=128,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20721], 17 s, 1 iter, 0.802 acc\n",
      " - train_5_batch_size=4,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20719], 14 s, 1 iter, 0.802 acc\n",
      " - train_6_batch_size=16,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20720], 8 s, 1 iter, 0.79 acc\n",
      " - train_7_batch_size=32,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20718], 13 s, 1 iter, 0.79 acc\n",
      " - train_8_batch_size=64,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20755], 9 s, 1 iter, 0.765 acc\n",
      " - train_9_batch_size=128,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20756], 12 s, 1 iter, 0.765 acc\n",
      " - train_10_batch_size=4,num_fc_layers=3:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20760], 19 s, 1 iter, 0.79 acc\n",
      " - train_11_batch_size=16,num_fc_layers=3:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20763], 10 s, 1 iter, 0.778 acc\n",
      " - train_12_batch_size=32,num_fc_layers=3:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20776], 9 s, 1 iter, 0.79 acc\n",
      " - train_13_batch_size=64,num_fc_layers=3:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20777], 9 s, 1 iter, 0.79 acc\n",
      " - train_14_batch_size=128,num_fc_layers=3:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20783], 10 s, 1 iter, 0.79 acc\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=20783)\u001b[0m WARNING: Not monitoring node memory since `psutil` is not installed. Install this with `pip install psutil` (or ray[debug]) to enable debugging of memory-related crashes.\n",
      "\u001b[2m\u001b[36m(pid=20796)\u001b[0m /Users/benmackenzie/projects/Teradata/ludwig/venv/lib/python3.6/site-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "\u001b[2m\u001b[36m(pid=20796)\u001b[0m   \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
      "Result for train_15_batch_size=4,num_fc_layers=4:\n",
      "  date: 2019-03-14_09-15-03\n",
      "  done: true\n",
      "  experiment_id: 944c4fd178394dd4bbf434907a99667c\n",
      "  hostname: 192.168.219.94\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.7654320987654321\n",
      "  node_ip: 192.168.219.94\n",
      "  pid: 20785\n",
      "  time_since_restore: 13.20129108428955\n",
      "  time_this_iter_s: 13.20129108428955\n",
      "  time_total_s: 13.20129108428955\n",
      "  timestamp: 1552569303\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/4 CPUs, 0/0 GPUs\n",
      "Unknown memory usage. Please run `pip install psutil` (or ray[debug]) to resolve)\n",
      "Result logdir: /Users/benmackenzie/ray_results/my_experiment\n",
      "Number of trials: 20 ({'TERMINATED': 16, 'RUNNING': 3, 'PENDING': 1})\n",
      "PENDING trials:\n",
      " - train_19_batch_size=128,num_fc_layers=4:\tPENDING\n",
      "RUNNING trials:\n",
      " - train_16_batch_size=16,num_fc_layers=4:\tRUNNING\n",
      " - train_17_batch_size=32,num_fc_layers=4:\tRUNNING\n",
      " - train_18_batch_size=64,num_fc_layers=4:\tRUNNING\n",
      "TERMINATED trials:\n",
      " - train_0_batch_size=4,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20716], 11 s, 1 iter, 0.79 acc\n",
      " - train_1_batch_size=16,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20714], 8 s, 1 iter, 0.802 acc\n",
      " - train_2_batch_size=32,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20715], 9 s, 1 iter, 0.802 acc\n",
      " - train_3_batch_size=64,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20717], 12 s, 1 iter, 0.815 acc\n",
      " - train_4_batch_size=128,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20721], 17 s, 1 iter, 0.802 acc\n",
      " - train_5_batch_size=4,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20719], 14 s, 1 iter, 0.802 acc\n",
      " - train_6_batch_size=16,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20720], 8 s, 1 iter, 0.79 acc\n",
      " - train_7_batch_size=32,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20718], 13 s, 1 iter, 0.79 acc\n",
      " - train_8_batch_size=64,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20755], 9 s, 1 iter, 0.765 acc\n",
      " - train_9_batch_size=128,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20756], 12 s, 1 iter, 0.765 acc\n",
      " - train_10_batch_size=4,num_fc_layers=3:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20760], 19 s, 1 iter, 0.79 acc\n",
      " - train_11_batch_size=16,num_fc_layers=3:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20763], 10 s, 1 iter, 0.778 acc\n",
      " - train_12_batch_size=32,num_fc_layers=3:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20776], 9 s, 1 iter, 0.79 acc\n",
      " - train_13_batch_size=64,num_fc_layers=3:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20777], 9 s, 1 iter, 0.79 acc\n",
      " - train_14_batch_size=128,num_fc_layers=3:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20783], 10 s, 1 iter, 0.79 acc\n",
      " - train_15_batch_size=4,num_fc_layers=4:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20785], 13 s, 1 iter, 0.765 acc\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=20785)\u001b[0m WARNING: Not monitoring node memory since `psutil` is not installed. Install this with `pip install psutil` (or ray[debug]) to enable debugging of memory-related crashes.\n",
      "\u001b[2m\u001b[36m(pid=20797)\u001b[0m /Users/benmackenzie/projects/Teradata/ludwig/venv/lib/python3.6/site-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "\u001b[2m\u001b[36m(pid=20797)\u001b[0m   \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
      "\u001b[2m\u001b[36m(pid=20802)\u001b[0m /Users/benmackenzie/projects/Teradata/ludwig/venv/lib/python3.6/site-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "\u001b[2m\u001b[36m(pid=20802)\u001b[0m   \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
      "Result for train_16_batch_size=16,num_fc_layers=4:\n",
      "  date: 2019-03-14_09-15-13\n",
      "  done: true\n",
      "  experiment_id: f0b8c69ccce34cb5a19c788089efc9e9\n",
      "  hostname: 192.168.219.94\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.7777777777777778\n",
      "  node_ip: 192.168.219.94\n",
      "  pid: 20796\n",
      "  time_since_restore: 10.057231187820435\n",
      "  time_this_iter_s: 10.057231187820435\n",
      "  time_total_s: 10.057231187820435\n",
      "  timestamp: 1552569313\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/4 CPUs, 0/0 GPUs\n",
      "Unknown memory usage. Please run `pip install psutil` (or ray[debug]) to resolve)\n",
      "Result logdir: /Users/benmackenzie/ray_results/my_experiment\n",
      "Number of trials: 20 ({'TERMINATED': 17, 'RUNNING': 3})\n",
      "RUNNING trials:\n",
      " - train_17_batch_size=32,num_fc_layers=4:\tRUNNING\n",
      " - train_18_batch_size=64,num_fc_layers=4:\tRUNNING\n",
      " - train_19_batch_size=128,num_fc_layers=4:\tRUNNING\n",
      "TERMINATED trials:\n",
      " - train_0_batch_size=4,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20716], 11 s, 1 iter, 0.79 acc\n",
      " - train_1_batch_size=16,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20714], 8 s, 1 iter, 0.802 acc\n",
      " - train_2_batch_size=32,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20715], 9 s, 1 iter, 0.802 acc\n",
      " - train_3_batch_size=64,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20717], 12 s, 1 iter, 0.815 acc\n",
      " - train_4_batch_size=128,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20721], 17 s, 1 iter, 0.802 acc\n",
      " - train_5_batch_size=4,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20719], 14 s, 1 iter, 0.802 acc\n",
      " - train_6_batch_size=16,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20720], 8 s, 1 iter, 0.79 acc\n",
      " - train_7_batch_size=32,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20718], 13 s, 1 iter, 0.79 acc\n",
      " - train_8_batch_size=64,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20755], 9 s, 1 iter, 0.765 acc\n",
      " - train_9_batch_size=128,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20756], 12 s, 1 iter, 0.765 acc\n",
      " - train_10_batch_size=4,num_fc_layers=3:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20760], 19 s, 1 iter, 0.79 acc\n",
      " - train_11_batch_size=16,num_fc_layers=3:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20763], 10 s, 1 iter, 0.778 acc\n",
      " - train_12_batch_size=32,num_fc_layers=3:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20776], 9 s, 1 iter, 0.79 acc\n",
      " - train_13_batch_size=64,num_fc_layers=3:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20777], 9 s, 1 iter, 0.79 acc\n",
      " - train_14_batch_size=128,num_fc_layers=3:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20783], 10 s, 1 iter, 0.79 acc\n",
      " - train_15_batch_size=4,num_fc_layers=4:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20785], 13 s, 1 iter, 0.765 acc\n",
      " - train_16_batch_size=16,num_fc_layers=4:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20796], 10 s, 1 iter, 0.778 acc\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=20796)\u001b[0m WARNING: Not monitoring node memory since `psutil` is not installed. Install this with `pip install psutil` (or ray[debug]) to enable debugging of memory-related crashes.\n",
      "Result for train_17_batch_size=32,num_fc_layers=4:\n",
      "  date: 2019-03-14_09-15-14\n",
      "  done: true\n",
      "  experiment_id: 2d25a2aa1fa94fb4b772b70fb7924063\n",
      "  hostname: 192.168.219.94\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.7777777777777778\n",
      "  node_ip: 192.168.219.94\n",
      "  pid: 20797\n",
      "  time_since_restore: 10.037584781646729\n",
      "  time_this_iter_s: 10.037584781646729\n",
      "  time_total_s: 10.037584781646729\n",
      "  timestamp: 1552569314\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=20797)\u001b[0m WARNING: Not monitoring node memory since `psutil` is not installed. Install this with `pip install psutil` (or ray[debug]) to enable debugging of memory-related crashes.\n",
      "\u001b[2m\u001b[36m(pid=20801)\u001b[0m /Users/benmackenzie/projects/Teradata/ludwig/venv/lib/python3.6/site-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "\u001b[2m\u001b[36m(pid=20801)\u001b[0m   \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
      "Result for train_18_batch_size=64,num_fc_layers=4:\n",
      "  date: 2019-03-14_09-15-18\n",
      "  done: true\n",
      "  experiment_id: 665a6eb26c3b4b0ab3cceb4806473119\n",
      "  hostname: 192.168.219.94\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.8024691358024691\n",
      "  node_ip: 192.168.219.94\n",
      "  pid: 20802\n",
      "  time_since_restore: 11.04942011833191\n",
      "  time_this_iter_s: 11.04942011833191\n",
      "  time_total_s: 11.04942011833191\n",
      "  timestamp: 1552569318\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=20802)\u001b[0m WARNING: Not monitoring node memory since `psutil` is not installed. Install this with `pip install psutil` (or ray[debug]) to enable debugging of memory-related crashes.\n",
      "Result for train_19_batch_size=128,num_fc_layers=4:\n",
      "  date: 2019-03-14_09-15-34\n",
      "  done: true\n",
      "  experiment_id: 7a473c4ec21c4b009a505d5fec8cd11d\n",
      "  hostname: 192.168.219.94\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.7901234567901234\n",
      "  node_ip: 192.168.219.94\n",
      "  pid: 20801\n",
      "  time_since_restore: 17.111340761184692\n",
      "  time_this_iter_s: 17.111340761184692\n",
      "  time_total_s: 17.111340761184692\n",
      "  timestamp: 1552569334\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/4 CPUs, 0/0 GPUs\n",
      "Unknown memory usage. Please run `pip install psutil` (or ray[debug]) to resolve)\n",
      "Result logdir: /Users/benmackenzie/ray_results/my_experiment\n",
      "Number of trials: 20 ({'TERMINATED': 20})\n",
      "TERMINATED trials:\n",
      " - train_0_batch_size=4,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20716], 11 s, 1 iter, 0.79 acc\n",
      " - train_1_batch_size=16,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20714], 8 s, 1 iter, 0.802 acc\n",
      " - train_2_batch_size=32,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20715], 9 s, 1 iter, 0.802 acc\n",
      " - train_3_batch_size=64,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20717], 12 s, 1 iter, 0.815 acc\n",
      " - train_4_batch_size=128,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20721], 17 s, 1 iter, 0.802 acc\n",
      " - train_5_batch_size=4,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20719], 14 s, 1 iter, 0.802 acc\n",
      " - train_6_batch_size=16,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20720], 8 s, 1 iter, 0.79 acc\n",
      " - train_7_batch_size=32,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20718], 13 s, 1 iter, 0.79 acc\n",
      " - train_8_batch_size=64,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20755], 9 s, 1 iter, 0.765 acc\n",
      " - train_9_batch_size=128,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20756], 12 s, 1 iter, 0.765 acc\n",
      " - train_10_batch_size=4,num_fc_layers=3:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20760], 19 s, 1 iter, 0.79 acc\n",
      " - train_11_batch_size=16,num_fc_layers=3:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20763], 10 s, 1 iter, 0.778 acc\n",
      " - train_12_batch_size=32,num_fc_layers=3:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20776], 9 s, 1 iter, 0.79 acc\n",
      " - train_13_batch_size=64,num_fc_layers=3:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20777], 9 s, 1 iter, 0.79 acc\n",
      " - train_14_batch_size=128,num_fc_layers=3:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20783], 10 s, 1 iter, 0.79 acc\n",
      " - train_15_batch_size=4,num_fc_layers=4:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20785], 13 s, 1 iter, 0.765 acc\n",
      " - train_16_batch_size=16,num_fc_layers=4:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20796], 10 s, 1 iter, 0.778 acc\n",
      " - train_17_batch_size=32,num_fc_layers=4:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20797], 10 s, 1 iter, 0.778 acc\n",
      " - train_18_batch_size=64,num_fc_layers=4:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20802], 11 s, 1 iter, 0.802 acc\n",
      " - train_19_batch_size=128,num_fc_layers=4:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20801], 17 s, 1 iter, 0.79 acc\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/4 CPUs, 0/0 GPUs\n",
      "Unknown memory usage. Please run `pip install psutil` (or ray[debug]) to resolve)\n",
      "Result logdir: /Users/benmackenzie/ray_results/my_experiment\n",
      "Number of trials: 20 ({'TERMINATED': 20})\n",
      "TERMINATED trials:\n",
      " - train_0_batch_size=4,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20716], 11 s, 1 iter, 0.79 acc\n",
      " - train_1_batch_size=16,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20714], 8 s, 1 iter, 0.802 acc\n",
      " - train_2_batch_size=32,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20715], 9 s, 1 iter, 0.802 acc\n",
      " - train_3_batch_size=64,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20717], 12 s, 1 iter, 0.815 acc\n",
      " - train_4_batch_size=128,num_fc_layers=1:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20721], 17 s, 1 iter, 0.802 acc\n",
      " - train_5_batch_size=4,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20719], 14 s, 1 iter, 0.802 acc\n",
      " - train_6_batch_size=16,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20720], 8 s, 1 iter, 0.79 acc\n",
      " - train_7_batch_size=32,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20718], 13 s, 1 iter, 0.79 acc\n",
      " - train_8_batch_size=64,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20755], 9 s, 1 iter, 0.765 acc\n",
      " - train_9_batch_size=128,num_fc_layers=2:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20756], 12 s, 1 iter, 0.765 acc\n",
      " - train_10_batch_size=4,num_fc_layers=3:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20760], 19 s, 1 iter, 0.79 acc\n",
      " - train_11_batch_size=16,num_fc_layers=3:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20763], 10 s, 1 iter, 0.778 acc\n",
      " - train_12_batch_size=32,num_fc_layers=3:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20776], 9 s, 1 iter, 0.79 acc\n",
      " - train_13_batch_size=64,num_fc_layers=3:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20777], 9 s, 1 iter, 0.79 acc\n",
      " - train_14_batch_size=128,num_fc_layers=3:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20783], 10 s, 1 iter, 0.79 acc\n",
      " - train_15_batch_size=4,num_fc_layers=4:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20785], 13 s, 1 iter, 0.765 acc\n",
      " - train_16_batch_size=16,num_fc_layers=4:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20796], 10 s, 1 iter, 0.778 acc\n",
      " - train_17_batch_size=32,num_fc_layers=4:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20797], 10 s, 1 iter, 0.778 acc\n",
      " - train_18_batch_size=64,num_fc_layers=4:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20802], 11 s, 1 iter, 0.802 acc\n",
      " - train_19_batch_size=128,num_fc_layers=4:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=20801], 17 s, 1 iter, 0.79 acc\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[train_0_batch_size=4,num_fc_layers=1,\n",
       " train_1_batch_size=16,num_fc_layers=1,\n",
       " train_2_batch_size=32,num_fc_layers=1,\n",
       " train_3_batch_size=64,num_fc_layers=1,\n",
       " train_4_batch_size=128,num_fc_layers=1,\n",
       " train_5_batch_size=4,num_fc_layers=2,\n",
       " train_6_batch_size=16,num_fc_layers=2,\n",
       " train_7_batch_size=32,num_fc_layers=2,\n",
       " train_8_batch_size=64,num_fc_layers=2,\n",
       " train_9_batch_size=128,num_fc_layers=2,\n",
       " train_10_batch_size=4,num_fc_layers=3,\n",
       " train_11_batch_size=16,num_fc_layers=3,\n",
       " train_12_batch_size=32,num_fc_layers=3,\n",
       " train_13_batch_size=64,num_fc_layers=3,\n",
       " train_14_batch_size=128,num_fc_layers=3,\n",
       " train_15_batch_size=4,num_fc_layers=4,\n",
       " train_16_batch_size=16,num_fc_layers=4,\n",
       " train_17_batch_size=32,num_fc_layers=4,\n",
       " train_18_batch_size=64,num_fc_layers=4,\n",
       " train_19_batch_size=128,num_fc_layers=4]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=20801)\u001b[0m WARNING: Not monitoring node memory since `psutil` is not installed. Install this with `pip install psutil` (or ray[debug]) to enable debugging of memory-related crashes.\n"
     ]
    }
   ],
   "source": [
    "from ray.tune import register_trainable, grid_search, run_experiments\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "grid_search_space = {\n",
    "    'num_fc_layers': grid_search([1,2,3,4]),\n",
    "    'batch_size': grid_search([4,16,32,64,128])\n",
    "}\n",
    "\n",
    "register_trainable('train', lambda cfg, rptr: train(base_model, cfg, rptr))\n",
    "run_experiments({'my_experiment': {\n",
    "    'run': 'train',\n",
    "    'stop': {'mean_accuracy': 0.9},\n",
    "    'config': grid_search_space}\n",
    "    })\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a more eleborate approach to specifying parameters to search over.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#version 1, doesn't handle lists.  extracts parameters that we want to search over.  \n",
    "\n",
    "\n",
    "import re\n",
    "pattern = \"^{{.*}}\"\n",
    "\n",
    "def get_keys(dct, path=\"\"):\n",
    "    parameters = []\n",
    "    for k, v in dct.items():\n",
    "        if isinstance(dct[k], dict):\n",
    "            p = get_keys(dct[k], k+\"->\" )\n",
    "            if p:\n",
    "                for l in p:\n",
    "                    parameters.append(l)\n",
    "            \n",
    "        elif isinstance(dct[k], str):\n",
    "            if re.match(pattern, dct[k], flags=0) is not None:\n",
    "                parameters.append(path + k +\"->\" + dct[k])\n",
    "       \n",
    "    return parameters\n",
    "\n",
    "#updates parameters...doens't handle lists\n",
    "\n",
    "def update_param(dct, path, value):\n",
    "    if len(path) == 1:\n",
    "        dct[path[0]] = value\n",
    "    else:\n",
    "        update_param(dct[path[0]], path[1:], value)\n",
    "        \n",
    "def build_model(base_model, config):\n",
    "    for k, v in config.items():\n",
    "        p = k.split('->')\n",
    "        update_param(base_model, p, v)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['optimizer->beta2->{{0.999}}', 'epochs->{{100}}', 'decay_rate->{{0.96}}']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training = {'optimizer': {'type': 'adam', 'beta1': 0.9, 'beta2': '{{0.999}}', 'epsilon': 1e-08},\n",
    " 'epochs': '{{100}}',\n",
    " 'regularizer': 'l2',\n",
    " 'regularization_lambda': 0,\n",
    " 'learning_rate': 0.001,\n",
    " 'batch_size': 128,\n",
    " 'dropout_rate': 0.0,\n",
    " 'early_stop': 5,\n",
    " 'reduce_learning_rate_on_plateau': 0,\n",
    " 'reduce_learning_rate_on_plateau_patience': 5,\n",
    " 'reduce_learning_rate_on_plateau_rate': 0.5,\n",
    " 'increase_batch_size_on_plateau': 0,\n",
    " 'increase_batch_size_on_plateau_patience': 5,\n",
    " 'increase_batch_size_on_plateau_rate': 2,\n",
    " 'increase_batch_size_on_plateau_max': 512,\n",
    " 'decay': False,\n",
    " 'decay_steps': 10000,\n",
    " 'decay_rate': '{{0.96}}',\n",
    " 'staircase': False,\n",
    " 'gradient_clipping': None,\n",
    " 'validation_field': 'combined',\n",
    " 'validation_measure': 'loss',\n",
    " 'bucketing_field': None,\n",
    " 'learning_rate_warmup_epochs': 5}\n",
    "\n",
    "\n",
    "k = get_keys(training)\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'optimizer': {'type': 'adam',\n",
       "  'beta1': 0.77,\n",
       "  'beta2': '{{0.999}}',\n",
       "  'epsilon': 1e-08},\n",
       " 'epochs': 999,\n",
       " 'regularizer': 'l2',\n",
       " 'regularization_lambda': 0,\n",
       " 'learning_rate': 0.001,\n",
       " 'batch_size': 128,\n",
       " 'dropout_rate': 0.0,\n",
       " 'early_stop': 5,\n",
       " 'reduce_learning_rate_on_plateau': 0,\n",
       " 'reduce_learning_rate_on_plateau_patience': 5,\n",
       " 'reduce_learning_rate_on_plateau_rate': 0.5,\n",
       " 'increase_batch_size_on_plateau': 0,\n",
       " 'increase_batch_size_on_plateau_patience': 5,\n",
       " 'increase_batch_size_on_plateau_rate': 2,\n",
       " 'increase_batch_size_on_plateau_max': 512,\n",
       " 'decay': False,\n",
       " 'decay_steps': 10000,\n",
       " 'decay_rate': '{{0.96}}',\n",
       " 'staircase': False,\n",
       " 'gradient_clipping': None,\n",
       " 'validation_field': 'combined',\n",
       " 'validation_measure': 'loss',\n",
       " 'bucketing_field': None,\n",
       " 'learning_rate_warmup_epochs': 5}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {'optimizer->beta1': 0.77, 'epochs': 999}\n",
    "build_model(training, config)\n",
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
